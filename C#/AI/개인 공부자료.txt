18년 딥러닝 https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU 
https://cs229.stanford.edu/syllabus-autumn2018.html 참고자료 
22년 딥러닝 https://www.youtube.com/watch?v=Bl4Feh_Mjvo&list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy
EM 알고리즘 - 비지도학습
1강 기계학습의 정의

명시적으로 프로그래밍을 하지않고 컴퓨터가 학습할수 있는 능력
특정 유형에서는 인간보다 더 뛰어 날수가 있다 음성인식, 바둑과 같은 좁은 형식의 규칙으로 하는경우에는
지도 학습에서 x,y 의 매핑관계를 측정하지만 가장쉬운 방식은 직선에 대하여 x,y값을 매핑하는경우 가장 단순한 매핑값일수 있으나 
회귀문제 예시 
회귀는 연속적인 경우에 적용 > x 집평, y가 가격 
분류는 y가 (1,0) > 이것이 참인지 거짓인지 데이터를 분류하는것 예시 > 이것이 악성종양인지 종양인지 > 이런경우 y가 실수값 


2강 선형 희귀 모델 (선형모델)
자율주행의 경우는 학습 문제가 회귀모델  
선형회귀모델의 수식화 
경사하강법의 공식화
경사하강법의 타원화된 도식화된 함수에서 하강 경사법으로 가파른 경사는 항상 90이며 직교한다.
너무 경사하강법을 반복화한다면 반복되는 계산이 많기에 최소화하기위해 몇단계를 넘어가는게 좋은 방법
비용함수가 증가한다 > 이러한경우 학습률이 크다는 신호값 
경사 하강법의 문제점은 너무 많은 데이터가 있다면 모든 데이터를 합산하여 계산하기때문에 학습효율에는 떨어질수있다 
1억개 이상의 사례인경우 사용하지 않는것이 좋다 경사하강법의 모든 단일단계에서 전체 데이터세트를 다 읽어와야한다는점
수렴하기위해 수백번의 스캔이 발생하는경우에는 너무 많은 비용함수가 든다 > 그것을 개선한 알고리즘 
확률적 경사 하강법

3강 로지스틱 희귀 
지역 가중 회귀 선형회귀 확률론을 적용하면 뉴턴의 로지스틱 회귀로 계산할수도있다 
x_i , y_i 로  n+1 차원의 행렬로 나타낼수있다 2가지 종류로한다면 3차원형으로 반환된다는 소리이며 , 회귀의 갯수는 실수계가 된다 
j는 손실함수 
지역 가중 , 로지스틱 회귀 
파라미터 알고리즘 훈련 규모가 아무리커도 훈련세트가 매개변수에 적합하다
논파라미터 알고리즘 데이터와 매개변수가 늘어날때 데이터 크기 훈련 세트 크기에 따라 선형적으로 증가한다

지역회귀 알고리즘의 핵심은 직선을 값을 추론하기위해서는 직선에 가까운값을 적용시키는것이 좋다  > x_i 함수는 가중치 함수
0 <(x_i - x) < 1 작다면 x에 가장 가까운 예제다 
x_i - x > 1 가중치가 0에 가까워야한다는 예시가 있다면 만약 x_i가 예측하려는 위치보다 멀리 떨어져 있다면 0을곱하거나 0에가장 가까운 수를 곱한다 > 반면 너무 가까운경우 1을 곱한다 
0을 곱한경우는 제곱에 오류가 사라진다 예측하려는 x에 가깝다 > 높이 만큼 가중치를 주는게 좋다
(그렇다면 미분을사용하여 직선에 가깝게 하는게 맞지않는가?) > 아쉽게도 직선을 만들어서 계산하는게아닌 가깝게하는 목적이기때문에 
미분을 한다고해서 좋은 데이터를 가질수 있는것은 아니다
KDTree 구현하면서 생각해보자 >> 오늘하고 나서 계속해보기 

14 강 
비지도학습은 정해지지않는 데이터 x,y 를 값을 주는것이 지도학습이지만, 비지도에서는 x 값만 주는것이 비지도의 핵심 

K-Means 군집 중심을 2개를 잡고 클러스트를 추정하는것 

K-Mean 평균 반복형 두가지를 반복적으로 값을 계산 

K-Mean 

첫번째는 클러스터링 초기화 
단 고차원적인 데이터를 할때는 무작위가아닌 정확한 하나의 기준점으로 계산하여 평균을 잡게된다

두번째는 수렴할때까지 반복 

비지도학습은 모호하지만 

노이즈가 있는 클러스트  
적절한 클러스터링의 갯수를 모호하나 
AIC(Akaike Information Criterion) , BIC(Bayesian Information Criterion)로 기준으로 공식을 사용해서 클러스터링 갯수를 추정할수있으나 

AIC(Akaike Information Criterion) = -2log L + 2K
BIC(Bayesian Information Criterion) = -2log L + K log n 
값이 작아질 수록 가장 정확한 모델링 갯수 
그러나 일반적으로는 K-Mean 모형에서는 다운 스트림으로 하기때문에 AIC, BIC를 사용하지않는데 앤드류 경우
지정되지 않는 데이터 세트가 있는 경우 x의 모델값으로 p(x)를 지정하여 계산한다면 특정 패턴값으로 추정할수있음

모델링 있는 알고리즘에서 p(X) 
ElBow Method 이유 > 두가지 이상의 가우스가 있는경에도 정확한 가우스값추정치가 없어도 p(x)값에 피팅이 가능하기때문에 사용 > 팔꿈치 모델이유


혼합 가우시안 모델 
